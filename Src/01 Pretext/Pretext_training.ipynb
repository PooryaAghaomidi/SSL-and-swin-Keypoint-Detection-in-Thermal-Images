{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deaafd35",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c383b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390dae26",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e4c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Conv2D, Flatten\n",
    "from tensorflow.keras.layers import AveragePooling2D, BatchNormalization, Conv2DTranspose\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from livelossplot.inputs.tf_keras import PlotLossesCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4398d48f",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f4343-28e2-4e3e-9827-9737444075f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = []\n",
    "my_subjects = {}\n",
    "counter = 0\n",
    "\n",
    "files = os.listdir('F:/poorya/datasets/keypoint J dataset/pretext dataset/train_aug/')\n",
    "\n",
    "for i in files:\n",
    "    sub.append(i[-6:-4])\n",
    "\n",
    "for i in range(95):\n",
    "    if str(i).zfill(2) in sub:\n",
    "        my_subjects[str(i).zfill(2)] = counter\n",
    "        counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ded773-a9da-483f-a8df-be3b8a5edc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, folder, batch_size, my_subjects):\n",
    "        self.folder = folder\n",
    "        self.n = os.listdir(folder)\n",
    "        self.batch_size = batch_size\n",
    "        self.my_subjects = my_subjects\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.n) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        list_IDs_temp = [self.n[k] for k in indexes]\n",
    "        x, y = self.__data_generation(list_IDs_temp)\n",
    "        return x, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.n))\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        images = np.empty((self.batch_size, 384, 384, 1))\n",
    "        masks = np.empty((self.batch_size, 48, 48, 1))\n",
    "        rotation = np.empty((self.batch_size), dtype = int)\n",
    "        subject = np.empty((self.batch_size), dtype = int)\n",
    "        gender = np.empty((self.batch_size), dtype = int)\n",
    "\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "\n",
    "            train_img_1 = image.img_to_array(image.load_img(self.folder + ID, color_mode = \"grayscale\", target_size = (\n",
    "                384, 384)))\n",
    "            train_img_1 /= 255.0\n",
    "            masks[i,] = train_img_1[168:216, 168:216]\n",
    "\n",
    "            train_img_2 = image.img_to_array(image.load_img(self.folder + ID, color_mode = \"grayscale\", target_size = (\n",
    "                384, 384)))\n",
    "            train_img_2 /= 255.0\n",
    "            train_img_2[168:216, 168:216] = 0\n",
    "            images[i,] = train_img_2\n",
    "\n",
    "            rotation[i] = int(ID[-10])\n",
    "            subject[i] = int(self.my_subjects[ID[-6:-4]])\n",
    "            gender[i] = int(ID[-8])\n",
    "\n",
    "        return images, [to_categorical(gender, num_classes = 2), to_categorical(rotation, num_classes = 4),\n",
    "                        to_categorical(subject, num_classes = 89), masks]\n",
    "\n",
    "\n",
    "bs = 16\n",
    "train_gen = DataGenerator('F:/poorya/datasets/keypoint J dataset/pretext dataset/train_aug/', bs, my_subjects)\n",
    "val_gen = DataGenerator('F:/poorya/datasets/keypoint J dataset/pretext dataset/val/', bs, my_subjects)\n",
    "test_gen = DataGenerator('F:/poorya/datasets/keypoint J dataset/pretext dataset/test/', bs, my_subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13ca300",
   "metadata": {},
   "source": [
    "## model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb70422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(layer_in, n_filters, n_conv):\n",
    "    for _ in range(n_conv):\n",
    "        layer_in = Conv2D(n_filters, (3, 3), padding = 'same', activation = 'relu')(layer_in)\n",
    "    layer_in = BatchNormalization()(layer_in)\n",
    "    layer_in = AveragePooling2D((2, 2), strides = (2, 2))(layer_in)\n",
    "    return layer_in\n",
    "\n",
    "\n",
    "def PSNR(super_resolution, high_resolution):\n",
    "    psnr_value = tf.image.psnr(high_resolution, super_resolution, max_val = 1)[0]\n",
    "    return psnr_value\n",
    "\n",
    "\n",
    "def SSIMLoss(y_true, y_pred):\n",
    "    return 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, 1.0))\n",
    "\n",
    "\n",
    "def get_model():\n",
    "\n",
    "    ######################################  INITIALIZATION  ########################################\n",
    "\n",
    "    loss_list = {'gender': 'categorical_crossentropy', 'rotation': 'categorical_crossentropy', 'subject': 'categorical_crossentropy', 'mask': 'mse'}\n",
    "    test_metrics = {'gender': 'accuracy', 'rotation': 'accuracy', 'subject': 'accuracy', 'mask': PSNR}\n",
    "    opt = tf.keras.optimizers.RMSprop(learning_rate = 0.01)\n",
    "    drp = 0.1\n",
    "    act = 'relu'\n",
    "    eact = 'softmax'\n",
    "    s = 'same'\n",
    "\n",
    "    ########################################  BACKBONE  ############################################\n",
    "\n",
    "    model_input = Input(shape = (384, 384, 1))\n",
    "\n",
    "    layer = vgg_block(model_input, 8, 2)\n",
    "    layer = vgg_block(layer, 16, 2)\n",
    "    layer = vgg_block(layer, 32, 2)\n",
    "    layer = vgg_block(layer, 64, 2)\n",
    "    layer = vgg_block(layer, 128, 2)\n",
    "    layer = vgg_block(layer, 256, 2)\n",
    "\n",
    "    ###################################  CLASSIFICATIONS  ##########################################\n",
    "\n",
    "    x = Flatten()(layer)\n",
    "    x = Dense(512, activation = act)(x)\n",
    "    x = Dropout(drp)(x)\n",
    "\n",
    "    y1 = Dense(256, activation = act)(x)\n",
    "    y1 = Dropout(drp)(y1)\n",
    "    y1 = Dense(128, activation = act)(y1)\n",
    "    y1 = Dropout(drp)(y1)\n",
    "    y1 = Dense(2, activation = eact, name = 'gender')(y1)\n",
    "\n",
    "    y2 = Dense(256, activation = act)(x)\n",
    "    y2 = Dropout(drp)(y2)\n",
    "    y2 = Dense(128, activation = act)(y2)\n",
    "    y2 = Dropout(drp)(y2)\n",
    "    y2 = Dense(4, activation = eact, name = 'rotation')(y2)\n",
    "\n",
    "    y3 = Dense(256, activation = act)(x)\n",
    "    y3 = Dropout(drp)(y3)\n",
    "    y3 = Dense(128, activation = act)(y3)\n",
    "    y3 = Dropout(drp)(y3)\n",
    "    y3 = Dense(89, activation = eact, name = 'subject')(y3)\n",
    "\n",
    "    ########################################  DECODER  ############################################\n",
    "\n",
    "    y4 = Conv2DTranspose(256, (3, 3), strides = (2, 2), activation = act, padding = s)(layer)\n",
    "    y4 = Conv2D(256, (3, 3), activation = act, padding = s)(y4)\n",
    "    y4 = Conv2D(256, (3, 3), activation = act, padding = s)(y4)\n",
    "    y4 = BatchNormalization()(y4)\n",
    "    y4 = Dropout(0.5)(y4)\n",
    "\n",
    "    y4 = Conv2DTranspose(64, (3, 3), strides = (2, 2), activation = act, padding = s)(y4)\n",
    "    y4 = Conv2D(64, (3, 3), activation = act, padding = s)(y4)\n",
    "    y4 = Conv2D(64, (3, 3), activation = act, padding = s)(y4)\n",
    "    y4 = BatchNormalization()(y4)\n",
    "    y4 = Dropout(0.5)(y4)\n",
    "\n",
    "    y4 = Conv2DTranspose(16, (3, 3), strides = (2, 2), activation = act, padding = s)(y4)\n",
    "    y4 = Conv2D(16, (3, 3), activation = act, padding = s)(y4)\n",
    "    y4 = Conv2D(16, (3, 3), activation = act, padding = s)(y4)\n",
    "    y4 = BatchNormalization()(y4)\n",
    "    y4 = Dropout(0.5)(y4)\n",
    "\n",
    "    y4 = Conv2D(1, (1, 1), padding = s, name = 'mask')(y4)\n",
    "\n",
    "    model = Model(inputs = model_input, outputs = [y1, y2, y3, y4])\n",
    "    model.compile(loss = loss_list, optimizer = opt, metrics = test_metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25629e32",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr():\n",
    "    chk_loss = ModelCheckpoint(filepath = 'checkpoint_loss6', monitor = 'val_loss', mode = 'min', verbose = 1, save_best_only = True)\n",
    "    early_st = EarlyStopping(monitor = \"val_loss\", patience = 30, verbose = 1, mode = \"min\")\n",
    "    rduce_lr = ReduceLROnPlateau(monitor = \"val_subject_loss\", factor = 0.5, patience = 5, verbose = 1, mode = \"min\", min_lr = 0.000001)\n",
    "    tr_plot = PlotLossesCallback()\n",
    "    lgg = tf.keras.callbacks.CSVLogger('kp_epoches.csv')\n",
    "    return [chk_loss, rduce_lr, tr_plot]\n",
    "\n",
    "\n",
    "cll = tr()\n",
    "history = model.fit(train_gen,\n",
    "                    validation_data = val_gen,\n",
    "                    batch_size = bs,\n",
    "                    epochs = 200,\n",
    "                    verbose = 1,\n",
    "                    callbacks = cll,\n",
    "                    steps_per_epoch = len(os.listdir('F:/poorya/datasets/keypoint J dataset/pretext dataset/train_aug/')) // bs,\n",
    "                    validation_steps = len(os.listdir('F:/poorya/datasets/keypoint J dataset/pretext dataset/val/')) // bs\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03900c3a",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b4da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodel_loss = load_model('checkpoint_loss6', custom_objects = {'PSNR': PSNR, 'SSIMLoss': SSIMLoss})\n",
    "testmodel_loss.evaluate(test_gen, steps = len(os.listdir('F:/poorya/datasets/keypoint J dataset/pretext dataset/val/')) // bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c87d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testmodel = load_model('checkpoint')\n",
    "folder = 'F:/poorya/datasets/keypoint J dataset/pretext dataset/test/'\n",
    "i = 1\n",
    "n = os.listdir(folder)\n",
    "n.sort(key = len)\n",
    "\n",
    "images = []\n",
    "mask = []\n",
    "rotation = []\n",
    "gender = []\n",
    "subject = []\n",
    "\n",
    "train_img_1 = image.img_to_array(image.load_img(folder + n[i], color_mode = \"grayscale\", target_size = (384, 384)))\n",
    "train_img_2 = image.img_to_array(image.load_img(folder + n[i], color_mode = \"grayscale\", target_size = (384, 384)))\n",
    "train_img_1 /= 255.0\n",
    "train_img_2 /= 255.0\n",
    "\n",
    "mask.append(train_img_1[168:216, 168:216])\n",
    "train_img_2[168:216, 168:216] = (0)\n",
    "images.append(train_img_2)\n",
    "\n",
    "rotation.append(to_categorical(int(n[i][-10]), num_classes = 4))\n",
    "subject.append(to_categorical(int(n[i][-6:-4]) - 1, num_classes = 89))\n",
    "gender.append(to_categorical(int(n[i][-8]), num_classes = 2))\n",
    "y1_pred, y2_pred, y3_pred, y4_pred = model.predict(np.array(images))\n",
    "\n",
    "plt.imshow(np.array(mask)[0, :, :, :])\n",
    "plt.show()\n",
    "plt.imshow(y4_pred[0, :, :, :])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poorya-tf24 kernel",
   "language": "python",
   "name": "poorya-tf24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
